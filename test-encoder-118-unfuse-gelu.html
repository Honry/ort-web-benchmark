<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>ONNXRuntime Web Benchmark Tool</title>
  </head>
  <style>
    body {
      font-family: sans-serif;
      padding: 20px;
    }

    h1 {
      color: #425066;
      font-size: 31px;
      margin-top: 0;
    }

    .loading-stats {
      color: #aaa;
      font-size: 12px;
      margin-top: -12px;
    }

    .hide {
      display: none;
    }

    .content {
      margin-top: 30px;
    }

    div {
      margin-top: 20px;
    }
  </style>
  <body>
    <h1>ONNXRuntime Web Benchmark Tool</h1>

    <!-- Loading status -->
    <div class="loading-stats">Choose options then click 'Run'...</div>
    <div>
      Number of runs:
      <input
        type="number"
        id="numRuns"
        value="1"
        min="1"
        defaultValue="1"
      />
    </div>
    <div>
      Model:
      <select id="model">
        <option value="whisper_base_encoder_lm_fp16_layernorm_gelu">whisper_base_encoder_lm_fp16_layernorm_gelu</option>
        <option value="whisper_base_decoder_static_non_kvcache_lm_fp16_layernorm_gelu_4dmask">whisper_base_decoder_static_non_kvcache_lm_fp16_layernorm_gelu_4dmask</option>
        <option value="whisper_base_decoder_static_kvcache_128_lm_fp16_layernorm_gelu_4dmask">whisper_base_decoder_static_kvcache_128_lm_fp16_layernorm_gelu_4dmask</option>
        <option value="whisper_base_decoder_static_non_kvcache_lm_fp16_layernorm_gelu">whisper_base_decoder_static_non_kvcache_lm_fp16_layernorm_gelu_2dmask</option>
        <option value="whisper_base_decoder_static_kvcache_128_lm_fp16_layernorm_gelu">whisper_base_decoder_static_kvcache_128_lm_fp16_layernorm_gelu_2dmask</option>
      </select>
    </div>
    <div>
      Backend:
      <select id="backend">
        <option value="webnn">webnn</option>
      </select>
    </div>
    <div>
      DeviceType:
      <select id="deviceType">
        <option value="cpu">WebNN CPU</option>
        <option value="gpu">WebNN GPU</option>
        <option value="npu" selected>WebNN NPU</option>
      </select>
      WebNN numThreads:
      <input type="number" id="numThreads" defaultValue="0" value="0" />
    </div>
    <div>
      <input type="button" value="Run" id="run" />
    </div>
    <div id="status" style="font: 1em sans-serif"></div>
    <!-- <script src="./onnxruntime-web/dist-1.18-unfuse-gelu/ort.all.js"></script> -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/onnxruntime-web/1.18.0/ort.all.min.js"
      integrity="sha512-MHOBvmgua1hQ3KTf3WRm+6UwgeulioSsl38T264/q8+zuL+Z0g0k9+VjvWC3lxAWtENRzUz4Bri9zKsZ/LRZ8w=="
      crossorigin="anonymous" referrerpolicy="no-referrer"></script>
    <script>
      function log(i) {
        console.log(i);
        document.getElementById("status").innerText +=
          `\n[${performance.now().toFixed(3)}] ` + i;
      }

      function generateTensor(dataType, shape, val) {
        let size = 1;
        shape.forEach((element) => {
          size *= element;
        });
        switch (dataType) {
          case "bool":
            return new ort.Tensor(
              dataType,
              Uint8Array.from({ length: size }, () => val),
              shape
            );
          case "uint8":
            return new ort.Tensor(
              dataType,
              Uint8Array.from({ length: size }, () => val),
              shape
            );
          case "uint16":
            return new ort.Tensor(
              dataType,
              Uint16Array.from({ length: size }, () => val),
              shape
            );
          case "float16":
            return new ort.Tensor(
              dataType,
              Uint16Array.from({ length: size }, () => val),
              shape
            );
          case "float32":
            return new ort.Tensor(
              dataType,
              Float32Array.from({ length: size }, () => val),
              shape
            );
          case "int32":
            return new ort.Tensor(
              dataType,
              Int32Array.from({ length: size }, () => val),
              shape
            );
          case "int64":
            return new ort.Tensor(
              dataType,
              BigInt64Array.from({ length: size }, () => val),
              shape
            );
        }
        throw new Error(`Input tensor type ${dataType} is unknown`);
      }

      const type_to_func = {
        bool: Uint8Array,
        uint8: Uint8Array,
        float32: Float32Array,
        uint16: Uint16Array,
        float16: Uint16Array,
        int32: Int32Array,
        BigInt64Array: BigInt64Array,
        int64: BigInt64Array,
      };

      function clone(x) {
        let feed = {};
        for (const [key, value] of Object.entries(x)) {
          let func = type_to_func[value.type];
          let arrayType = func.from(value.data);
          feed[key] = new ort.Tensor(
            value.type,
            arrayType.slice(0),
            value.dims
          );
        }
        return feed;
      }

      ort.env.wasm.numThreads = 1;
      ort.env.wasm.simd = true;
      ort.env.wasm.proxy = false;
      ort.env.logLevel = "warning"; //"error";
      ort.env.debug = false;
      // ort.env.wasm.wasmPaths = location.origin + "/onnxruntime-web/ort-web-20240114/dist/";

      async function run() {
        let feed = {};
        const provider = document.getElementById("backend").value;
        const modelName = document.getElementById("model").value;
        const deviceType = document.getElementById("deviceType").value;
        const numThreads = document.getElementById("numThreads").value;
        log("entering run ...");

        try {
          const options = {
            executionProviders: [
              {
                name: provider,
                deviceType: deviceType,
                powerPreference: "default",
                numThreads: parseInt(numThreads),
              },
            ],
            //executionProviders: [{name: "webnn", deviceType: 'gpu', powerPreference: 'high-performance' }],
          };
          const urlPrefix = location.href.includes('github.io') ? 'https://huggingface.co/lwanming/whisper-base-static-shape/resolve/main/' : 'models/';
          const modelPath = await getModelOPFS(modelName, `${urlPrefix + modelName}.onnx`, false);
          if (modelName.includes('whisper_base_encoder')) {
            feed["input_features"] = generateTensor("float16", [1, 80, 3000], 0.5);
          } else if (modelName.includes('whisper_base_decoder_static_non_kvcache_lm_fp16')) {
            feed['input_ids'] = generateTensor('int32', [1,4], 1);
            if (modelName.indexOf('4dmask') != -1) {
              feed['attention_mask'] = generateTensor('float16', [1,1,4,4], 1);
            } else {
              feed['attention_mask'] = generateTensor('int32', [1,4], 1);
            }

            feed['encoder_hidden_states'] = generateTensor('float16', [1,1500,512], 1);
          } else if (modelName.includes('whisper_base_decoder_static_kvcache_128_lm_fp16')) {
            feed['input_ids'] = generateTensor('int32', [1,1], 1);
            if (modelName.indexOf('4dmask') != -1) {
              feed['attention_mask'] = generateTensor('float16', [1,1,1,128], 1);
            } else {
              feed['attention_mask'] = generateTensor('int64', [1,128], 1n);
            }
            feed['position_ids'] = generateTensor('int32', [1], 1);
            feed['past_key_values.0.decoder.key'] = generateTensor("float16", [1, 8, 127, 64], 0.5);
            feed['past_key_values.0.decoder.value'] = generateTensor("float16", [1, 8, 127, 64], 0.5);
            feed['past_key_values.0.encoder.key'] = generateTensor("float16", [1, 8, 1500, 64], 0.5);
            feed['past_key_values.0.encoder.value'] = generateTensor("float16", [1, 8, 1500, 64], 0.5);
            feed['past_key_values.1.decoder.key'] = generateTensor("float16", [1, 8, 127, 64], 0.5);
            feed['past_key_values.1.decoder.value'] = generateTensor("float16", [1, 8, 127, 64], 0.5);
            feed['past_key_values.1.encoder.key'] = generateTensor("float16", [1, 8, 1500, 64], 0.5);
            feed['past_key_values.1.encoder.value'] = generateTensor("float16", [1, 8, 1500, 64], 0.5);
            feed['past_key_values.2.decoder.key'] = generateTensor("float16", [1, 8, 127, 64], 0.5);
            feed['past_key_values.2.decoder.value'] = generateTensor("float16", [1, 8, 127, 64], 0.5);
            feed['past_key_values.2.encoder.key'] = generateTensor("float16", [1, 8, 1500, 64], 0.5);
            feed['past_key_values.2.encoder.value'] = generateTensor("float16", [1, 8, 1500, 64], 0.5);
            feed['past_key_values.3.decoder.key'] = generateTensor("float16", [1, 8, 127, 64], 0.5);
            feed['past_key_values.3.decoder.value'] = generateTensor("float16", [1, 8, 127, 64], 0.5);
            feed['past_key_values.3.encoder.key'] = generateTensor("float16", [1, 8, 1500, 64], 0.5);
            feed['past_key_values.3.encoder.value'] = generateTensor("float16", [1, 8, 1500, 64], 0.5);
            feed['past_key_values.4.decoder.key'] = generateTensor("float16", [1, 8, 127, 64], 0.5);
            feed['past_key_values.4.decoder.value'] = generateTensor("float16", [1, 8, 127, 64], 0.5);
            feed['past_key_values.4.encoder.key'] = generateTensor("float16", [1, 8, 1500, 64], 0.5);
            feed['past_key_values.4.encoder.value'] = generateTensor("float16", [1, 8, 1500, 64], 0.5);
            feed['past_key_values.5.decoder.key'] = generateTensor("float16", [1, 8, 127, 64], 0.5);
            feed['past_key_values.5.decoder.value'] = generateTensor("float16", [1, 8, 127, 64], 0.5);
            feed['past_key_values.5.encoder.key'] = generateTensor("float16", [1, 8, 1500, 64], 0.5);
            feed['past_key_values.5.encoder.value'] = generateTensor("float16", [1, 8, 1500, 64], 0.5);
          } else {
            throw new Error(`Model ${modelName} is unknown`);
          }

          options.logSeverityLevel = 0;

          log("creating session ...");
          console.log("sessionOptions: ", options);
          const sess_start = performance.now();
          const sess = await ort.InferenceSession.create(modelPath, options);
          log(`Session created in ${(performance.now() - sess_start).toFixed(2)} ms`);
          log("warmup ...");
          console.log("inputs: ", feed);
          const output = await sess.run(clone(feed));

          console.log("output: ", output);
          log("running ...");
          let N = document.getElementById("numRuns").value;
          N = N === null ? 10 : parseInt(N);
          if (N < 1) {
            throw new Error("Run Number should be greater than 0!");
          }
          let inferTimes = [];

          for (var i = 0; i < N; i++) {
            const start = performance.now();
            console.time("ort session.run()");
            const outputs = await sess.run(clone(feed));
            console.timeEnd("ort session.run()");
            //const outputs = await sess.run(feed); // Without clone(), you get DOMException: Failed to execute 'postMessage' on 'Worker': ArrayBuffer at index 0 is already detached.
            inferTimes.push(performance.now() - start);
          }
          // sess.endProfiling();
          let intermediateTimings = "";
          for (var i = 0; i < N; i++) {
            intermediateTimings += `${inferTimes[i].toFixed(2)}, `;
          }
          log(intermediateTimings);
          const totalTime = inferTimes.reduce(
            (partialSum, a) => partialSum + a,
            0
          );
          const result = `${modelName}/${provider}, ${(totalTime / N).toFixed(
            2
          )} ms / iter`;
          log(result);
        } catch (e) {
          log(e);
        }
      }
      // Get model via Origin Private File System
async function getModelOPFS(name, url, updateModel) {
  const root = await navigator.storage.getDirectory();
  let fileHandle;

  async function updateFile() {
    const response = await fetch(url);
    const buffer = await readResponse(response);
    fileHandle = await root.getFileHandle(name, {create: true});
    const writable = await fileHandle.createWritable();
    await writable.write(buffer);
    await writable.close();
    return buffer;
  }

  if (updateModel) {
    return await updateFile();
  }

  try {
    fileHandle = await root.getFileHandle(name);
    const blob = await fileHandle.getFile();
    return await blob.arrayBuffer();
  } catch (e) {
    return await updateFile();
  }
}

async function readResponse(response) {
  const contentLength = response.headers.get('Content-Length');
  let total = parseInt(contentLength ?? '0');
  let buffer = new Uint8Array(total);
  let loaded = 0;

  const reader = response.body.getReader();
  async function read() {
    const {done, value} = await reader.read();
    if (done) return;

    let newLoaded = loaded + value.length;
    if (newLoaded > total) {
      total = newLoaded;
      let newBuffer = new Uint8Array(total);
      newBuffer.set(buffer);
      buffer = newBuffer;
    }
    buffer.set(value, loaded);
    loaded = newLoaded;
    return read();
  }

  await read();
  return buffer;
}
      const runBtn = document.getElementById("run");
      runBtn.onclick = async () => {
        await run();
      };
    </script>
  </body>
</html>
